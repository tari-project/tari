
########################################################################################################################
#                                                                                                                      #
#                      Base Node Configuration Options (BaseNodeConfig)                                                #
#                                                                                                                      #
########################################################################################################################

# If you are not running a Tari Base node, you can simply leave everything in this section commented out. Base nodes
# help maintain the security of the Tari token and are the surest way to preserve your privacy and be 100% sure that
# no-one is cheating you out of your money.

[dibbler.base_node]
# A path to the file that stores your node identity and secret key (default = "config/base_node_id.json")
identity_file = "config/base_node_id_dibbler.json"

[igor.base_node]
# A path to the file that stores your node identity and secret key (default = "config/base_node_id.json")
identity_file = "config/base_node_id_igor.json"

# The socket to expose for the gRPC base node server (default = "/ip4/127.0.0.1/tcp/18152")
#grpc_address = "/ip4/127.0.0.1/tcp/18152"

[esmeralda.base_node]
# A path to the file that stores your node identity and secret key (default = "config/base_node_id.json")
identity_file = "config/base_node_id_esmeralda.json"

# The socket to expose for the gRPC base node server (default = "/ip4/127.0.0.1/tcp/18142")
#grpc_address = "/ip4/127.0.0.1/tcp/18142"

[base_node]
# Selected network (Note: Not implemented properly, please specify on the command line) (default = "emseralda")
#network = "emseralda"

# Set to false to disable the base node GRPC server (default = true)
#grpc_enabled = true

# A path to the file that stores your node identity and secret key (default = "config/base_node_id.json")
#identity_file = "config/base_node_id.json"

# Spin up and use a built-in Tor instance, only works on macos/linux and must comment out 'tor.control_address' below.
# This requires that the base node was built with the optional "libtor" feature flag. (default = false)
#use_libtor = false

# A path to the file that stores the tor hidden service private key, if using the tor transport.
# (default = "config/tor_id.json")
#tor_identity_file = "config/base_node_tor_id.json"

# The type of database backend to use. Currently supported options are "memory" and "lmdb". LMDB is recommnded for
# almost all use cases. (default = "lmdb")
#db_type = "lmdb"

# The relative path to store persistent data (default = "data/base_node")
#data_dir = "data/base_node"

# The relative path to store the lmbd data (default = "db")
#lmdb_path = "db"

# The maximum amount of VMs that RandomX will be use (default = 5)
#max_randomx_vms = 5

# Bypass range proof verification to speed up validation (default = false)
#bypass_range_proof_verification = false

# This allowlist provides a method to force syncing from any known nodes you may choose, for example if you have a
# couple of nodes that you always want to have in sync. If set this node will only sync to the nodes in this set.
# force_sync_peers = ["public_key1::address1", "public_key2::address2",... ]

# The maximum amount of seconds wait for remote base node responses for messaging-based requests.
#messaging_request_timeout = 60

# The time interval between status line updates in the CLI (default = 5 s)
#status_line_interval = 5

# The buffer size constants for the publish/subscribe connector channel, connecting comms messages to the domain layer:
# (min value = 30, default value = 1500).
#buffer_size = 1500

# The rate limit constants for the publish/subscribe connector channel, i.e. maximum amount of inbound messages to
# accept - any rate attempting to exceed this limit will be throttled (min value = 5, default value = 1000).
#buffer_rate_limit = 1000

# Liveness meta data auto ping interval between peers (default = 30 s)
#metadata_auto_ping_interval = 30

# Obscure GRPC error responses (default = false)
#report_grpc_error = false

[base_node.lmdb]
#init_size_bytes = 16_777_216 # 16 *1024 * 1024
#grow_size_bytes = 16_777_216 # 16 *1024 * 1024
#resize_threshold_bytes = 4_194_304 # 4 *1024 * 1024

[base_node.storage]
# The maximum number of orphans that can be stored in the Orphan block pool.
#orphan_storage_capacity = 720
# The pruning horizon that is set for a default configuration of the blockchain db.
#pruning_horizon = 0
# The chain height interval used to determine when a pruned node should perform pruning.
#pruning_interval = 50
# Set to true to record all reorgs. Recorded reorgs can be viewed using the list-reorgs command. Default = false
track_reorgs = true
# Clean out
#cleanup_orphans_at_startup = false

[base_node.mempool]
# The maximum number of transactions that can be stored in the Unconfirmed Transaction pool
#unconfirmed_pool.storage_capacity = 40_000
# The maximum number of transactions that can be skipped when compiling a set of highest priority transactions,
# skipping over large transactions are performed in an attempt to fit more transactions into the remaining space.
#unconfirmed_pool.weight_tx_skip_count = 20

# The height horizon to clear transactions from the reorg pool.
#reorg_pool.expiry_height = 5

# Number of peers from which to initiate a sync. Once this many peers have successfully synced, this node will
# not initiate any more mempool syncs. Default: 2
#service.initial_sync_num_peers = 2
# The maximum number of transactions to sync in a single sync session Default: 10_000
#service.initial_sync_max_transactions = 10_000
# The maximum number of blocks added via sync or re-org to triggering a sync
#service.block_sync_trigger = 5

[base_node.state_machine]
# The initial max sync latency. If a peer fails to stream a header/block within this deadline another sync peer will be
# selected. If there are no further peers the sync will be restarted with an increased by `max_latency_increase`.
#blockchain_sync_config.initial_max_sync_latency = 10
# If all sync peers exceed latency increase allowed latency by this value
#blockchain_sync_config.max_latency_increase =2
# Longer ban period for potentially malicious infractions (protocol violations etc.)
#blockchain_sync_config.ban_period = 1_800 # 30 * 60
# Short ban period for infractions that are likely not malicious (slow to respond spotty connections etc)
#blockchain_sync_config.short_ban_period = 60
# An allowlist of sync peers from which to sync. No other peers will be selected for sync. If empty sync peers
# are chosen based on their advertised chain metadata.
#blockchain_sync_config.forced_sync_peers = []
# Number of threads to use for validation
#blockchain_sync_config.validation_concurrency = 6

# The maximum amount of VMs that RandomX will be use (default = 0)
#max_randomx_vms = 0
# The amount of blocks this node can be behind a peer before considered to be lagging (to test the block
# propagation by delaying lagging) (default = 0)
#blocks_behind_before_considered_lagging = 0
# Bypass range proof verification to speed up validation (default = false)
#bypass_range_proof_verification = false

[base_node.p2p]
# The node's publicly-accessible hostname. This is the host name that is advertised on the network so that
# peers can find you.
# _NOTE_: If using the `tor` transport type, public_address will be ignored and an onion address will be
# automatically configured
#public_address = "/ip4/172.2.3.4/tcp/18189"

# Optionally bind an additional TCP socket for inbound Tari P2P protocol commms.
# Use cases include:
# - allowing wallets to locally connect to their base node, rather than through tor, when used in conjunction with
#   `tor_proxy_bypass_addresses`
# - multiple P2P addresses, one public over DNS and one private over TOR
# - a "bridge" between TOR and TCP-only nodes
# auxiliary_tcp_listener_address = "/ip4/127.0.0.1/tcp/9998"

# Path to the LMDB data files
#datastore_path = "peer_db"

# Name to use for the peer database
#peer_database_name = "peers"

# The maximum number of concurrent Inbound tasks allowed before back-pressure is applied to peers
#max_concurrent_inbound_tasks = 4

# The maximum number of concurrent outbound tasks allowed before back-pressure is applied to outbound messaging queue
#max_concurrent_outbound_tasks = 4

# Set to true to allow peers to provide test addresses (loopback, memory etc.). If set to false, memory
# addresses, loopback, local-link (i.e addresses used in local tests) will not be accepted from peers. This
# should always be false for non-test nodes.
#allow_test_addresses = false

# CIDR for addresses allowed to enter into liveness check mode on the listener.
#listener_liveness_allowlist_cidrs = []
# Enables periodic socket-level liveness checks. Default: Disabled
listener_liveness_check_interval = 15

# User agent string for this node
#user_agent = ""

# The maximum simultaneous comms RPC sessions allowed (default value = 100). Setting this to -1 will allow unlimited
# sessions.
#rpc_max_simultaneous_sessions = 100
# The maximum comms RPC sessions allowed per peer (default value = 10).
#rpc_max_sessions_per_peer = 10

[base_node.p2p.transport]
# -------------- Transport configuration --------------
# Use TCP to connect to the Tari network. This transport can only communicate with TCP/IP addresses, so peers with
# e.g. tor onion addresses will not be contactable. (default = "tor")
#type = "tor"

# The address and port to listen for peer connections over TCP. (use: type = "tcp")
#tcp.listener_address = "/ip4/0.0.0.0/tcp/18189"
# Configures a tor proxy used to connect to onion addresses. All other traffic uses direct TCP connections.
# This setting is optional however, if it is not specified, this node will not be able to connect to nodes that
# only advertise an onion address. (default = )
#tcp.tor_socks_address =
# Optional tor SOCKS proxy authentication (default = "none")
#tcp.tor_socks_auth = "none"

# Configures the node to run over a tor hidden service using the Tor proxy. This transport recognises ip/tcp,
# onion v2, onion v3 and dns addresses. (use: type = "tor")
# Address of the tor control server
#tor.control_address = "/ip4/127.0.0.1/tcp/9051"
# SOCKS proxy auth (default = "none")
#tor.socks_auth = "none"
# Use this socks address instead of getting it from the tor proxy. (default = )
#tor.socks_address_override =
# Authentication to use for the tor control server (default = "auto")
#tor.control_auth = "auto" # or "password=xxxxxx"
# The onion port to use.
#tor.onion_port = 18141
# When these peer addresses are encountered when dialing another peer, the tor proxy is bypassed and the connection is
# made directly over TCP. /ip4, /ip6, /dns, /dns4 and /dns6 are supported. (e.g. ["/dns4/my-foo-base-node/tcp/9998"])
#tor.proxy_bypass_addresses = []
# When using the tor transport and set to true, outbound TCP connections bypass the tor proxy. Defaults to false for
# better privacy
#tor.proxy_bypass_for_outbound_tcp = false
# If set, instructs tor to forward traffic the the provided address. (e.g. "/ip4/127.0.0.1/tcp/0") (default = )
#tor.forward_address =

# Use a SOCKS5 proxy transport. This transport recognises any addresses supported by the proxy.
# (use: type = "socks5")
# The address of the SOCKS5 proxy. Traffic will be forwarded to tcp.listener_address.
# (Default = "/ip4/127.0.0.1/tcp/8080")
#socks.proxy_address = "/ip4/127.0.0.1/tcp/9050"
# SOCKS proxy auth (Default = "none", or assign "username_password=username:xxxxxxx")
#socks.auth = "none"

# Use a Memory proxy transport. (use: type = "memory")
#memory.listener_address = "/memory/0"

[base_node.p2p.dht]
# The `DbConnectionUrl` for the Dht database. Default: In-memory database
database_url = "data/base_node/dht.db"
# The size of the buffer (channel) which holds pending outbound message requests. Default: 20
#outbound_buffer_size = 20
# The maximum number of peer nodes that a message has to be closer to, to be considered a neighbour. Default: 8
#num_neighbouring_nodes = 8
# Number of random peers to include. Default: 4
#num_random_nodes=  4
# Send to this many peers when using the broadcast strategy. Default: 8
#broadcast_factor = 8
# Send to this many peers when using the propagate strategy. Default: 4
#propagation_factor = 4

# The amount of time added to the current time will be used to check if the message has expired or not. Default: 3 hours
#saf.msg_validity = 10_800 # 3 * 60 * 60 // 3 hours
# The maximum number of messages that can be stored using the Store-and-forward middleware. Default: 100,000
#saf.msg_storage_capacity = 100_000
# A request to retrieve stored messages will be ignored if the requesting node is not within one of this nodes _n_
# closest nodes. Default 10
#saf.num_closest_nodes = 10
# The maximum number of messages to return from a store and forward retrieval request. Default: 100
#saf.max_returned_messages = 50
# The time-to-live duration used for storage of low priority messages by the Store-and-forward middleware.
# Default: 6 hours
#saf.low_priority_msg_storage_ttl = 21_600 # 6 * 60 * 60 // 6 hours
# The time-to-live duration used for storage of high priority messages by the Store-and-forward middleware.
# Default: 3 days
#saf.high_priority_msg_storage_ttl = 259_200 # 3 * 24 * 60 * 60 // 3 days
# The limit on the message size to store in SAF storage in bytes. Default 500 KiB
#saf.max_message_size = 524_288 # 512 * 1024
# When true, store and forward messages are requested from peers on connect (Default: true)
#saf.auto_request = true
# The maximum allowed time between asking for a message and accepting a response
#saf.max_inflight_request_age = 120
# The maximum number of peer nodes that a message must be closer than to get stored by SAF. Default: 8
#saf.num_neighbouring_nodes = 8

# The max capacity of the message hash cache. Default: 2,500
#dedup_cache_capacity = 2_500
# The periodic trim interval for items in the message hash cache. Default: 300s (5 mins)
#dedup_cache_trim_interval = 300 # 5 * 60
# The number of occurrences of a message is allowed to pass through the DHT pipeline before being deduped/discarded
# Default: 1
#dedup_allowed_message_occurrences = 1
# The duration to wait for a peer discovery to complete before giving up. Default: 2 minutes
#discovery_request_timeout = 120 # 2 * 60
# Set to true to automatically broadcast a join message when ready, otherwise false. Default: false
#auto_join = true
# The minimum time between sending a Join message to the network. Joins are only sent when the node establishes
# enough connections to the network as determined by comms ConnectivityManager. If a join was sent and then state
# change happens again after this period, another join will be sent. Default: 10 minutes
#join_cooldown_interval = 120 # 10 * 60

# The interval to update the neighbouring and random pools, if necessary. Default: 2 minutes
#connectivity.update_interval = 120 # 2 * 60
# The interval to change the random pool peers. Default = 2 hours
#connectivity.random_pool_refresh_interval = 7_200 # 2 * 60 * 60
# Length of cooldown when high connection failure rates are encountered. Default: 45s
#connectivity.high_failure_rate_cooldown = 45
# The minimum desired ratio of TCPv4 to Tor connections. TCPv4 addresses have some significant cost to create,
# making sybil attacks costly. This setting does not guarantee this ratio is maintained.
# Currently, it only emits a warning if the ratio is below this setting. Default: 0.1 (10%)
#connectivity.minimum_desired_tcpv4_node_ratio = 0.1

# True to enable network discovery, false to disable it. Default: true
#network_discovery.enabled = true
# A threshold for the minimum number of peers this node should ideally be aware of. If below this threshold a
# more "aggressive" strategy is employed. Default: 50
#network_discovery.min_desired_peers = 50
# The period to wait once the number of rounds given by `idle_after_num_rounds` has completed. Default: 30 mins
#network_discovery.idle_period = 1_800 # 30 * 60
#  The minimum number of network discovery rounds to perform before idling (going to sleep). If there are less
#  than `min_desired_peers` then the actual number of rounds performed will exceed this value. Default: 10
#network_discovery.idle_after_num_rounds = 10
# Time to idle after a failed round. Default: 5 secs
#network_discovery.on_failure_idle_period = 5
# The maximum number of sync peer to select for each round. The selection strategy varies depending on the current state.
# Default: 5
#network_discovery.max_sync_peers = 5

# Length of time to ban a peer if the peer misbehaves at the DHT-level. Default: 6 hrs
#ban_duration = 21_600 # 6 * 60 * 60
# Length of time to ban a peer for a "short" duration. Default: 60 mins
#ban_duration_short = 3_600 # 60 * 60
# This allows the use of test addresses in the network like 127.0.0.1. Default: false
#allow_test_addresses = false
# The maximum number of messages over `flood_ban_timespan` to allow before banning the peer (for `ban_duration_short`)
# Default: 100_000 messages
#flood_ban_max_msg_count = 100_000
# The timespan over which to calculate the max message rate.
# `flood_ban_max_count / flood_ban_timespan (as seconds) = avg. messages per second over the timespan`
#  Default: 100 seconds
#flood_ban_timespan = 100
# Once a peer has been marked as offline, wait at least this length of time before reconsidering them.
# In a situation where a node is not well-connected and many nodes are locally marked as offline, we can retry
# peers that were previously tried. Default: 2 hours
#offline_peer_cooldown = 7_200 # 2 * 60 * 60
